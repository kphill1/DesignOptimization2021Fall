{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "harmful-logging",
   "metadata": {},
   "source": [
    "### Problem 1 (50 points) \n",
    "\n",
    "Vapor-liquid equilibria data are correlated using two adjustable parameters $A_{12}$ and $A_{21}$ per binary\n",
    "mixture. For low pressures, the equilibrium relation can be formulated as:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "p = & x_1\\exp\\left(A_{12}\\left(\\frac{A_{21}x_2}{A_{12}x_1+A_{21}x_2}\\right)^2\\right)p_{water}^{sat}\\\\\n",
    "& + x_2\\exp\\left(A_{21}\\left(\\frac{A_{12}x_1}{A_{12}x_1+A_{21}x_2}\\right)^2\\right)p_{1,4 dioxane}^{sat}.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Here the saturation pressures are given by the Antoine equation\n",
    "\n",
    "$$\n",
    "\\log_{10}(p^{sat}) = a_1 - \\frac{a_2}{T + a_3},\n",
    "$$\n",
    "\n",
    "where $T = 20$($^{\\circ}{\\rm C}$) and $a_{1,2,3}$ for a water - 1,4 dioxane\n",
    "system is given below.\n",
    "\n",
    "|             | $a_1$     | $a_2$      | $a_3$     |\n",
    "|:------------|:--------|:---------|:--------|\n",
    "| Water       | 8.07131 | 1730.63  | 233.426 |\n",
    "| 1,4 dioxane | 7.43155 | 1554.679 | 240.337 |\n",
    "\n",
    "\n",
    "The following table lists the measured data. Recall that in a binary system $x_1 + x_2 = 1$.\n",
    "\n",
    "|$x_1$ | 0.0 | 0.1 | 0.2 | 0.3 | 0.4 | 0.5 | 0.6 | 0.7 | 0.8 | 0.9 | 1.0 |\n",
    "|:-----|:--------|:---------|:--------|:-----|:-----|:-----|:-----|:-----|:-----|:-----|:-----|\n",
    "|$p$| 28.1 | 34.4 | 36.7 | 36.9 | 36.8 | 36.7 | 36.5 | 35.4 | 32.9 | 27.7 | 17.5 |\n",
    "\n",
    "Estimate $A_{12}$ and $A_{21}$ using data from the above table: \n",
    "\n",
    "1. Formulate the least square problem; \n",
    "2. Since the model is nonlinear, the problem does not have an analytical solution. Therefore, solve it using the gradient descent or Newton's method implemented in HW1; \n",
    "3. Compare your optimized model with the data. Does your model fit well with the data?\n",
    "\n",
    "---\n",
    "\n",
    "### Problem 2 (50 points) \n",
    "\n",
    "Solve the following problem using Bayesian Optimization:\n",
    "$$\n",
    "    \\min_{x_1, x_2} \\quad \\left(4-2.1x_1^2 + \\frac{x_1^4}{3}\\right)x_1^2 + x_1x_2 + \\left(-4 + 4x_2^2\\right)x_2^2,\n",
    "$$\n",
    "for $x_1 \\in [-3,3]$ and $x_2 \\in [-2,2]$. A tutorial on Bayesian Optimization can be found [here](https://thuijskens.github.io/2016/12/29/bayesian-optimisation/).\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "divine-setup",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0., -4.], dtype=float32)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A simple example of using PyTorch for gradient descent\n",
    "\n",
    "import torch as t\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# Define a variable, make sure requires_grad=True so that PyTorch can take gradient with respect to this variable\n",
    "x = Variable(t.tensor([1.0, 0.0]), requires_grad=True)\n",
    "\n",
    "# Define a loss\n",
    "loss = (x[0] - 1)**2 + (x[1] - 2)**2\n",
    "\n",
    "# Take gradient\n",
    "loss.backward()\n",
    "\n",
    "# Check the gradient. numpy() turns the variable from a PyTorch tensor to a numpy array.\n",
    "x.grad.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "positive-starter",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2., -6.], dtype=float32)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's examine the gradient at a different x.\n",
    "x.data = t.tensor([2.0, 1.0])\n",
    "loss = (x[0] - 1)**2 + (x[1] - 2)**2\n",
    "loss.backward()\n",
    "x.grad.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "infectious-remark",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'zero_'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-96-8173575086e9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[1;31m# need to clear the gradient at every step, or otherwise it will accumulate...\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m         \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'zero_'"
     ]
    }
   ],
   "source": [
    "# Here is a code for gradient descent without line search\n",
    "\n",
    "import torch as t\n",
    "from torch.autograd import Variable\n",
    "\n",
    "x = Variable(t.tensor([1.0, 0.0]), requires_grad=True)\n",
    "\n",
    "# Fix the step size\n",
    "a = 0.01\n",
    "\n",
    "# Start gradient descent\n",
    "for i in range(1000):  # TODO: change the termination criterion\n",
    "    loss = (x[0] - 1)**2 + (x[1] - 2)**2\n",
    "    loss.backward()\n",
    "    \n",
    "    # no_grad() specifies that the operations within this context are not part of the computational graph, i.e., we don't need the gradient descent algorithm itself to be differentiable with respect to x\n",
    "    with t.no_grad():\n",
    "        x -= a * x.grad\n",
    "        \n",
    "        # need to clear the gradient at every step, or otherwise it will accumulate...\n",
    "        x.grad.zero_()\n",
    "        \n",
    "print(x.data.numpy())\n",
    "print(loss.data.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "painful-climb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([13504447. , -5644399.5], dtype=float32)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch as t\n",
    "from torch.autograd import Variable\n",
    "import math\n",
    "# Defining variables we want to optimize\n",
    "#A12 = Variable(t.tensor([5.0]), requires_grad=True)\n",
    "#A21 = Variable(t.tensor([6.0]), requires_grad=True)\n",
    "A = Variable(t.tensor([5.0,6.0]), requires_grad=True)\n",
    "#A12 = 5\n",
    "#A21 = 6\n",
    "# Setting constants\n",
    "T = 20\n",
    "a1_w = 8.07131\n",
    "a2_w = 1730.63\n",
    "a3_w = 233.426\n",
    "a1_d = 7.43155\n",
    "a2_d = 1554.679\n",
    "a3_d = 240.337\n",
    "p_satw = pow(10,(a1_w - (a2_w)/(T+a3_w)))\n",
    "p_satd = pow(10,(a1_d - (a2_d)/(T+a3_d)))\n",
    "x1 = [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0]\n",
    "p = [28.1,34.4,36.7,36.9,36.8,36.7,36.5,35.4,32.9,27.7,17.5]\n",
    "#p = 28.1\n",
    "pd = lambda x1,A12,A21: x1*p_satw*math.exp(A12*((A21*(1-x1))/(A12*x1+A21*)(1-x1)**2) + (1-x1)*math.exp(A21*((A12*x1)/(A12*x1 + A21*(1-x1)))**2)*p_satd\n",
    "#x2 = 1.0\n",
    "for i in range(len(x1)):\n",
    "    if i == 0:\n",
    "        x2 = [1-x1[i]]\n",
    "        #pf[i] = pd(x1[i],x2[i],58492.12,-28337.963)\n",
    "    else:\n",
    "        x2.append(1-x1[i])\n",
    "        #pf[i] = pd(x1[i],x2[i],58492.12,-28337.963)\n",
    "        \n",
    "#A12c = 0\n",
    "#A21c = 0\n",
    "for i in range(len(x1)):\n",
    "    #if i == 0:\n",
    "    #loss = (p[i] - x1[i]*t.exp(A1(2*((A21*x2[i])/(A12*x1[i]+A21*x2[i]))**2)*p_satw + x2[i]*t.exp(A21*((A12*x2[i])/(A12*x1[i] + A21*x2[i]))**2)*p_satd)**2\n",
    "    loss = ((p[i] - x1[i]*p_satw*t.exp(A[0]*((A[1]*x2[i])/(A[0]*x1[i]+A[1]*x2[i]))**2) + x2[i]*p_satd*t.exp(A[1]*((A[0]*x2[i])/(A[0]*x1[i] + A[1]*x2[i]))**2))**2)\n",
    "    loss.backward()\n",
    "    #A12c = A12c+A12.grad.numpy()\n",
    "    #A21c = A21c+A21.grad.numpy()\n",
    "#A12.grad.numpy()\n",
    "#A21.grad.numpy()\n",
    "#loss = ((p[i] - x1[i]*p_satw*t.exp(A[0]*((A[1]*x2[i])/(A[0]*x1[i]+A[1]*x2[i]))**2) + x2[i]*p_satd*t.exp(A[1]*((A[0]*x2[i])/(A[0]*x1[i] + A[1]*x2[i]))**2))**2)\n",
    "A.grad.numpy()\n",
    "        #pd(x1,x2,A12.grad.numpy(),A21.grad.numpy())\n",
    "#loss = (p[1] - x1[1]*math.exp(A12.grad.numpy()*((A21.grad.numpy()*x2[1])/(A12.grad.numpy*x1[1]+A21.grad.numpy*x2[1]))**2)*p_satw + x2[1]*math.exp(A21.grad.numpy()*((A12.grad.numpy()*x2[1])/(A12.grad.numpy*x1[1] + A21.grad.numpy*x2[1]))**2)*p_satd)**2        \n",
    "    #else: \n",
    "    #loss = (p[i] - x1[i]*t.exp(A12*(A21*x2[i]/(A12*x1[i]+A21*x2[i]))**2)*p_satw + x2[i]*t.exp(A21*(A12*x2[i]/(A12*x1[i] + A21*x2[i]))**2)*p_satd)**2\n",
    "    #loss.backward()\n",
    "    #print(A12.grad.numpy())\n",
    "    #print(A21.grad.numpy())\n",
    "    #A12c = [A12.grad.numpy()]\n",
    "    #A21c = [A21.grad.numpy()]\n",
    "        #A12c.append(A12.grad.numpy())\n",
    "        #A21c.append(A21.grad.numpy())\n",
    "     #   print(A12.grad.numpy())\n",
    "      #  print(A21.grad.numpy())\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "1d3b8c86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 88217.516, 130706.11 ], dtype=float32)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch as t\n",
    "from torch.autograd import Variable\n",
    "import math\n",
    "A = Variable(t.tensor([5.0,6.0]), requires_grad=True)\n",
    "T = 20\n",
    "a1_w = 8.07131\n",
    "a2_w = 1730.63\n",
    "a3_w = 233.426\n",
    "a1_d = 7.43155\n",
    "a2_d = 1554.679\n",
    "a3_d = 240.337\n",
    "p_satw = pow(10,(a1_w - (a2_w)/(T+a3_w)))\n",
    "p_satd = pow(10,(a1_d - (a2_d)/(T+a3_d)))\n",
    "x1 = [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0]\n",
    "p = [28.1,34.4,36.7,36.9,36.8,36.7,36.5,35.4,32.9,27.7,17.5]\n",
    "pd = lambda x1,A12,A21: x1*t.exp(A12*((A12*(1-x1))/(A12*x1 + A21*(1-x1)))**2)*p_satw + (1-x1)*t.exp(A21*((A12*x1)/(A12*x1+A21*(1-x1)))**2)*p_satd\n",
    "for i in range(len(x1)):\n",
    "    if i == 0:\n",
    "        x2 = [1-x1[i]]\n",
    "    else:\n",
    "        x2.append(1-x1[i])    \n",
    "for i in range(len(x1)):\n",
    "    \n",
    "    #loss = t.norm((p[i] - (x1[i]*p_satw*t.exp(A[0]*((A[1]*x2[i])/(A[0]*x1[i]+A[1]*x2[i]))**2) + x2[i]*p_satd*t.exp(A[1]*((A[0]*x2[i])/(A[0]*x1[i] + A[1]*x2[i])**2))**2)\n",
    "    #loss = (x2*t.exp(A[0]*(A[2]*x2)))\n",
    "    loss = t.norm(p[i] - pd(x1[i],A[0],A[1]))**2\n",
    "    loss.backward()\n",
    "    \n",
    "A.grad.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "56a1eecb",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[-3.         -2.87755102 -2.75510204 -2.63265306 -2.51020408 -2.3877551\n -2.26530612 -2.14285714 -2.02040816 -1.89795918 -1.7755102  -1.65306122\n -1.53061224 -1.40816327 -1.28571429 -1.16326531 -1.04081633 -0.91836735\n -0.79591837 -0.67346939 -0.55102041 -0.42857143 -0.30612245 -0.18367347\n -0.06122449  0.06122449  0.18367347  0.30612245  0.42857143  0.55102041\n  0.67346939  0.79591837  0.91836735  1.04081633  1.16326531  1.28571429\n  1.40816327  1.53061224  1.65306122  1.7755102   1.89795918  2.02040816\n  2.14285714  2.26530612  2.3877551   2.51020408  2.63265306  2.75510204\n  2.87755102  3.        ].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-50-1b64bd7f9fba>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_iters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m         \u001b[1;31m# Sample next hyperparameter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\gaussian_process\\_gpr.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    190\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    191\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkernel_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequires_vector_input\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 192\u001b[1;33m             X, y = self._validate_data(X, y, multi_output=True, y_numeric=True,\n\u001b[0m\u001b[0;32m    193\u001b[0m                                        ensure_2d=True, dtype=\"numeric\")\n\u001b[0;32m    194\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    431\u001b[0m                 \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    432\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 433\u001b[1;33m                 \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    434\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    435\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m    812\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"y cannot be None\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    813\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 814\u001b[1;33m     X = check_array(X, accept_sparse=accept_sparse,\n\u001b[0m\u001b[0;32m    815\u001b[0m                     \u001b[0maccept_large_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maccept_large_sparse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    816\u001b[0m                     \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    635\u001b[0m             \u001b[1;31m# If input is 1D raise error\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    636\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 637\u001b[1;33m                 raise ValueError(\n\u001b[0m\u001b[0;32m    638\u001b[0m                     \u001b[1;34m\"Expected 2D array, got 1D array instead:\\narray={}.\\n\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    639\u001b[0m                     \u001b[1;34m\"Reshape your data either using array.reshape(-1, 1) if \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[-3.         -2.87755102 -2.75510204 -2.63265306 -2.51020408 -2.3877551\n -2.26530612 -2.14285714 -2.02040816 -1.89795918 -1.7755102  -1.65306122\n -1.53061224 -1.40816327 -1.28571429 -1.16326531 -1.04081633 -0.91836735\n -0.79591837 -0.67346939 -0.55102041 -0.42857143 -0.30612245 -0.18367347\n -0.06122449  0.06122449  0.18367347  0.30612245  0.42857143  0.55102041\n  0.67346939  0.79591837  0.91836735  1.04081633  1.16326531  1.28571429\n  1.40816327  1.53061224  1.65306122  1.7755102   1.89795918  2.02040816\n  2.14285714  2.26530612  2.3877551   2.51020408  2.63265306  2.75510204\n  2.87755102  3.        ].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sklearn.gaussian_process as gp\n",
    "\n",
    "from scipy.stats import norm\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "#kernel = gp.kernels.Matern()\n",
    "#model = gp.GaussianProcessRegressor(kernel=kernal, alpha=1e-4, n_restarts_optimizer=10, normaluze_y = True)\n",
    "\n",
    "x1 = np.linspace(-3,3)\n",
    "x2 = np.linspace(-2,2)\n",
    "n_iters = 4\n",
    "sample_loss = (4 - 2.1*x1**2 +(x1**4/3))*x1**2 + x1*x2 + (-4 + 4*x2**2)*x2**2\n",
    "bounds = np.array([[-3, 3], [-2,2]])\n",
    "n_pre_samples = 2\n",
    "random_search=False\n",
    "x0 =None\n",
    "x1_list = []\n",
    "x2_list = []\n",
    "gp_params = None\n",
    "n_params = bounds.shape[0]\n",
    "alpha = 1e-5\n",
    "\n",
    "if x0 is None:\n",
    "    for params in np.random.uniform(bounds[:, 0], bounds[:, 1], (n_pre_samples, bounds.shape[0])):\n",
    "        x1_list.append(params)\n",
    "        x2_list.append(sample_loss)\n",
    "else:\n",
    "    for params in x0:\n",
    "        x1_list.append(params)\n",
    "        x2_list.append(sample_loss)\n",
    "\n",
    "x1p = np.array(x1_list)\n",
    "x2p = np.array(x2_list)\n",
    "\n",
    "    # Create the GP\n",
    "if gp_params is not None:\n",
    "    model = gp.GaussianProcessRegressor(**gp_params)\n",
    "else:\n",
    "    kernel = gp.kernels.Matern()\n",
    "    model = gp.GaussianProcessRegressor(kernel=kernel,\n",
    "                                            alpha=alpha,\n",
    "                                            n_restarts_optimizer=10,\n",
    "                                            normalize_y=True)\n",
    "\n",
    "for n in range(n_iters):\n",
    "    model.fit(x1, x2)\n",
    "    \n",
    "        # Sample next hyperparameter\n",
    "    if random_search:\n",
    "        x_random = np.random.uniform(bounds[:, 0], bounds[:, 1], size=(random_search, n_params))\n",
    "        ei = -1 * expected_improvement(x_random, model, yp, greater_is_better=True, n_params=n_params)\n",
    "        next_sample = x_random[np.argmax(ei), :]\n",
    "    else:\n",
    "        next_sample = sample_next_hyperparameter(expected_improvement, model, yp, greater_is_better=True, bounds=bounds, n_restarts=100)\n",
    "\n",
    "        # Duplicates will break the GP. In case of a duplicate, we will randomly sample a next query point.\n",
    "    if np.any(np.abs(next_sample - xp) <= epsilon):\n",
    "        next_sample = np.random.uniform(bounds[:, 0], bounds[:, 1], bounds.shape[0])\n",
    "\n",
    "        # Sample loss for new set of parameters\n",
    "    cv_score = sample_loss(next_sample)\n",
    "\n",
    "        # Update lists\n",
    "    x1_list.append(next_sample)\n",
    "    x2_list.append(cv_score)\n",
    "\n",
    "        # Update xp and yp\n",
    "    x1 = np.array(x1_list)\n",
    "    x2 = np.array(x2_list)\n",
    "\n",
    "return x1, x2\n",
    "#bound = np.array([[-3, 3], [-2,2]])\n",
    "\n",
    "#x1, x2 = bayesian_optimisation(n_iters = 5,sample_loss = (4 - 2.1*x1^2 +(x1^4/3))*x1^2 + x1*x2 + (-4 + 4*x2^2)*x2^2, bounds=bound, n_pre_samples = 3, random_search=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "267ad3fb",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for ** or pow(): 'list' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-921cca2f7ae0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[0mx1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[0mx2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m \u001b[0mbayesian_optimization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_iters\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m4\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m2.1\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mx1\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m2\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mx1\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mx2\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx2\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx2\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx1\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0myp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx2\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for ** or pow(): 'list' and 'int'"
     ]
    }
   ],
   "source": [
    "import sklearn.gaussian_process as gp\n",
    "import numpy as np\n",
    "def bayesian_optimization(n_iters, sample_loss, xp, yp):\n",
    "    \n",
    "    # Define the GP\n",
    "    kernel = gp.kernels.Matern()\n",
    "    model = gp.GaussianProcessRegressor(kernel=kernel, alpha=1e-4,n_restarts_optimizer=10,normalize_y=True)\n",
    "    for i in range(n_iters):\n",
    "        # Update our belief of the loss function\n",
    "        model.fit(xp,yp)\n",
    "        \n",
    "        next_sample = sample_next_hyperparameter(model, yp)\n",
    "        \n",
    "        next_loss = sample_loss(next_sample)\n",
    "        \n",
    "        x_list.append(next_sample)\n",
    "        y_list.append(next_loss)\n",
    "        \n",
    "        xp = np.array(x_list)\n",
    "        yp = np.array(y_list)\n",
    "    return xy, yp\n",
    "x1 = [-3,-2,-1,0,1]\n",
    "x2 = [-2,-1,0,1,2]\n",
    "bayesian_optimization(n_iters = 5, sample_loss = (4 - 2.1*x1**2 + (x1**4)/3)*(x1**2) + x1*x2 + (-4+4*(x2**2))*(x2**2), xp = x1 , yp = x2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6cb49476",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expected_improvement(x, gaussian_process, evaluated_loss, greater_is_better=False, n_params = 1):\n",
    "     \n",
    "        x_to_predict = x.reshape(-1, n_params)\n",
    "        \n",
    "        mu, sigma = gaussian_process.predict(x_to_predict, return_std=True)\n",
    "    \n",
    "        if greater_is_better:\n",
    "            loss_optimum = np.max(evaluated_loss)\n",
    "        else:\n",
    "            loss_optimum = np.min(evaluated_loss)\n",
    "\n",
    "        scaling_factor = (-1) ** (not greater_is_better)\n",
    "\n",
    "    # In case sigma equals zero\n",
    "        with np.errstate(divide='ignore'):\n",
    "            Z = scaling_factor * (mu - loss_optimum) / sigma\n",
    "            expected_improvement = scaling_factor * (mu - loss_optimum) * norm.cdf(Z) + sigma * norm.pdf(Z)\n",
    "            expected_improvement[sigma == 0.0] == 0.0\n",
    "\n",
    "        return -1 * expected_improvement\n",
    "\n",
    "def sample_next_hyperparameter(acquisition_func, gaussian_process, evaluated_loss, greater_is_better=False,\n",
    "                               bounds=(0, 10), n_restarts=25):\n",
    "    \n",
    "        best_x = None\n",
    "        best_acquisition_value = 1\n",
    "        n_params = bounds.shape[0]\n",
    "\n",
    "        for starting_point in np.random.uniform(bounds[:, 0], bounds[:, 1], size=(n_restarts, n_params)):\n",
    "\n",
    "            res = minimize(fun=acquisition_func,\n",
    "                 x0=starting_point.reshape(1, -1),\n",
    "                 bounds=bounds,\n",
    "                 method='L-BFGS-B',\n",
    "                       args=(gaussian_process, evaluated_loss, greater_is_better, n_params))\n",
    "\n",
    "        if res.fun < best_acquisition_value:\n",
    "            best_acquisition_value = res.fun\n",
    "            best_x = res.x\n",
    "\n",
    "        return best_x\n",
    "\n",
    "\n",
    "# def bayesian_optimisation(n_iters, sample_loss, bounds, x0=None, n_pre_samples=5, gp_params=None, random_search=False, alpha=1e-5, epsilon=1e-7):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b5bc49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn.gaussian_process as gp\n",
    "\n",
    "from scipy.stats import norm\n",
    "from scipy.optimize import minimize\n",
    "value = [np.linspace(-3,3, num=5), np.linspace(-2,2, num=5)]\n",
    "bounds = np.array([[-3,3], [-2,2]])\n",
    "n_iters = 2\n",
    "sample_loss = lambda value: (4 - 2.1*value[0]**2 + (value[0]**4)/3)*(value[0]**2) + value[0]*value[1] + (-4+4*(value[1]**2))*(value[1]**2)\n",
    "x0 = None\n",
    "n_pre_samples = 5\n",
    "gp_params = None\n",
    "random_search = False\n",
    "alpha = 1e-5\n",
    "epsilon = 1e-7\n",
    "#def bayesian_optimisation(n_iters, sample_loss, bounds, x0=None, n_pre_samples=5,\n",
    "                          #gp_params=None, random_search=False, alpha=1e-5, epsilon=1e-7):\n",
    "    \n",
    "x_list = []\n",
    "y_list = []\n",
    "\n",
    "n_params = bounds.shape[0]\n",
    "\n",
    "if x0 is None:\n",
    "    for params in np.random.uniform(bounds[:, 0], bounds[:, 1], (n_pre_samples, bounds.shape[0])):\n",
    "        x_list.append(params)\n",
    "        y_list.append(sample_loss(params))\n",
    "else:\n",
    "    for params in x0:\n",
    "        x_list.append(params)\n",
    "        y_list.append(sample_loss(params))\n",
    "\n",
    "xp = np.array(x_list)\n",
    "yp = np.array(y_list)\n",
    "\n",
    "    # Create the GP\n",
    "if gp_params is not None:\n",
    "    model = gp.GaussianProcessRegressor(**gp_params)\n",
    "else:\n",
    "    kernel = gp.kernels.Matern()\n",
    "    model = gp.GaussianProcessRegressor(kernel=kernel,\n",
    "                                            alpha=alpha,\n",
    "                                            n_restarts_optimizer=10,\n",
    "                                            normalize_y=True)\n",
    "\n",
    "for n in range(n_iters):\n",
    "\n",
    "    model.fit(xp, yp)\n",
    "\n",
    "        # Sample next hyperparameter\n",
    "    if random_search:\n",
    "        x_random = np.random.uniform(bounds[:, 0], bounds[:, 1], size=(random_search, n_params))\n",
    "        ei = -1 * expected_improvement(x_random, model, yp, greater_is_better=True, n_params=n_params)\n",
    "        next_sample = x_random[np.argmax(ei), :]\n",
    "    else:\n",
    "        next_sample = sample_next_hyperparameter(expected_improvement, model, yp, greater_is_better=True, bounds=bounds, n_restarts=100)\n",
    "\n",
    "        # Duplicates will break the GP. In case of a duplicate, we will randomly sample a next query point.\n",
    "    if np.any(np.abs(next_sample - xp) <= epsilon):\n",
    "        next_sample = np.random.uniform(bounds[:, 0], bounds[:, 1], bounds.shape[0])\n",
    "\n",
    "        # Sample loss for new set of parameters\n",
    "    cv_score = sample_loss(next_sample)\n",
    "\n",
    "        # Update lists\n",
    "    x_list.append(next_sample)\n",
    "    y_list.append(cv_score)\n",
    "\n",
    "        # Update xp and yp\n",
    "    xp = np.array(x_list)\n",
    "    yp = np.array(y_list)\n",
    "\n",
    "xp, yp\n",
    "#return xp, yp\n",
    "#value = [np.linspace(-3,3, num=5), np.linspace(-2,2, num=5)]\n",
    "#x1 = np.linspace(-3,3, num=5)\n",
    "#x2 = np.linspace(-2,2, num=5)\n",
    "#sample_loss = lambda value: (4 - 2.1*value[0]**2 + (value[0]**4)/3)*(value[0]**2) + value[0]*value[1] + (-4+4*(value[1]**2))*(value[1]**2)\n",
    "\n",
    "#xp, yp = bayesian_optimisation(n_iters = 2, sample_loss = sample_loss(value), bounds = bounds, x0 = None, n_pre_samples = 5, gp_params = None, random_search = False, alpha = 1e-5, epsilon = 1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "7256d9ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.84880877,  1.64787545],\n",
       "        [ 2.90911266, -1.35712607],\n",
       "        [ 1.31059964,  1.63468531],\n",
       "        [-0.13292725, -1.84109917],\n",
       "        [-1.22940462,  1.33874441],\n",
       "        [ 3.        , -1.86369343],\n",
       "        [-0.24931714, -1.07728736]]),\n",
       " array([ 19.15140208,  87.74276266,  22.38028641,  32.71497689,\n",
       "          6.43299327, 137.67223638,   1.25448345]))"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xp, yp"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
