{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "harmful-logging",
   "metadata": {},
   "source": [
    "### Problem 1 (50 points) \n",
    "\n",
    "Vapor-liquid equilibria data are correlated using two adjustable parameters $A_{12}$ and $A_{21}$ per binary\n",
    "mixture. For low pressures, the equilibrium relation can be formulated as:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "p = & x_1\\exp\\left(A_{12}\\left(\\frac{A_{21}x_2}{A_{12}x_1+A_{21}x_2}\\right)^2\\right)p_{water}^{sat}\\\\\n",
    "& + x_2\\exp\\left(A_{21}\\left(\\frac{A_{12}x_1}{A_{12}x_1+A_{21}x_2}\\right)^2\\right)p_{1,4 dioxane}^{sat}.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Here the saturation pressures are given by the Antoine equation\n",
    "\n",
    "$$\n",
    "\\log_{10}(p^{sat}) = a_1 - \\frac{a_2}{T + a_3},\n",
    "$$\n",
    "\n",
    "where $T = 20$($^{\\circ}{\\rm C}$) and $a_{1,2,3}$ for a water - 1,4 dioxane\n",
    "system is given below.\n",
    "\n",
    "|             | $a_1$     | $a_2$      | $a_3$     |\n",
    "|:------------|:--------|:---------|:--------|\n",
    "| Water       | 8.07131 | 1730.63  | 233.426 |\n",
    "| 1,4 dioxane | 7.43155 | 1554.679 | 240.337 |\n",
    "\n",
    "\n",
    "The following table lists the measured data. Recall that in a binary system $x_1 + x_2 = 1$.\n",
    "\n",
    "|$x_1$ | 0.0 | 0.1 | 0.2 | 0.3 | 0.4 | 0.5 | 0.6 | 0.7 | 0.8 | 0.9 | 1.0 |\n",
    "|:-----|:--------|:---------|:--------|:-----|:-----|:-----|:-----|:-----|:-----|:-----|:-----|\n",
    "|$p$| 28.1 | 34.4 | 36.7 | 36.9 | 36.8 | 36.7 | 36.5 | 35.4 | 32.9 | 27.7 | 17.5 |\n",
    "\n",
    "Estimate $A_{12}$ and $A_{21}$ using data from the above table: \n",
    "\n",
    "1. Formulate the least square problem; \n",
    "2. Since the model is nonlinear, the problem does not have an analytical solution. Therefore, solve it using the gradient descent or Newton's method implemented in HW1; \n",
    "3. Compare your optimized model with the data. Does your model fit well with the data?\n",
    "\n",
    "---\n",
    "\n",
    "### Problem 2 (50 points) \n",
    "\n",
    "Solve the following problem using Bayesian Optimization:\n",
    "$$\n",
    "    \\min_{x_1, x_2} \\quad \\left(4-2.1x_1^2 + \\frac{x_1^4}{3}\\right)x_1^2 + x_1x_2 + \\left(-4 + 4x_2^2\\right)x_2^2,\n",
    "$$\n",
    "for $x_1 \\in [-3,3]$ and $x_2 \\in [-2,2]$. A tutorial on Bayesian Optimization can be found [here](https://thuijskens.github.io/2016/12/29/bayesian-optimisation/).\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "divine-setup",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0., -4.], dtype=float32)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A simple example of using PyTorch for gradient descent\n",
    "\n",
    "import torch as t\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# Define a variable, make sure requires_grad=True so that PyTorch can take gradient with respect to this variable\n",
    "x = Variable(t.tensor([1.0, 0.0]), requires_grad=True)\n",
    "\n",
    "# Define a loss\n",
    "loss = (x[0] - 1)**2 + (x[1] - 2)**2\n",
    "\n",
    "# Take gradient\n",
    "loss.backward()\n",
    "\n",
    "# Check the gradient. numpy() turns the variable from a PyTorch tensor to a numpy array.\n",
    "x.grad.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "positive-starter",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2., -6.], dtype=float32)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's examine the gradient at a different x.\n",
    "x.data = t.tensor([2.0, 1.0])\n",
    "loss = (x[0] - 1)**2 + (x[1] - 2)**2\n",
    "loss.backward()\n",
    "x.grad.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "infectious-remark",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.        1.9999971]\n",
      "8.185452e-12\n"
     ]
    }
   ],
   "source": [
    "# Here is a code for gradient descent without line search\n",
    "\n",
    "import torch as t\n",
    "from torch.autograd import Variable\n",
    "\n",
    "x = Variable(t.tensor([1.0, 0.0]), requires_grad=True)\n",
    "\n",
    "# Fix the step size\n",
    "a = 0.01\n",
    "\n",
    "# Start gradient descent\n",
    "for i in range(1000):  # TODO: change the termination criterion\n",
    "    loss = (x[0] - 1)**2 + (x[1] - 2)**2\n",
    "    loss.backward()\n",
    "    \n",
    "    # no_grad() specifies that the operations within this context are not part of the computational graph, i.e., we don't need the gradient descent algorithm itself to be differentiable with respect to x\n",
    "    with t.no_grad():\n",
    "        x -= a * x.grad\n",
    "        \n",
    "        # need to clear the gradient at every step, or otherwise it will accumulate...\n",
    "        x.grad.zero_()\n",
    "        \n",
    "print(x.data.numpy())\n",
    "print(loss.data.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "id": "1d3b8c86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([109826.984, 150164.86 ], dtype=float32)"
      ]
     },
     "execution_count": 370,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Formulate least square problem\n",
    "print(\"The function to be minimized is (p(x1,A12,A21) - p_i)**2 with respect to A12 and A21\")\n",
    "# Use pytorch to calculate gradient\n",
    "\n",
    "import torch as t\n",
    "from torch.autograd import Variable\n",
    "import math\n",
    "A = Variable(t.tensor([5.0,6.0]), requires_grad=True)\n",
    "T = 20\n",
    "a1_w = 8.07131\n",
    "a2_w = 1730.63\n",
    "a3_w = 233.426\n",
    "a1_d = 7.43155\n",
    "a2_d = 1554.679\n",
    "a3_d = 240.337\n",
    "p_satw = pow(10,(a1_w - (a2_w)/(T+a3_w)))\n",
    "\n",
    "p_satd = pow(10,(a1_d - (a2_d)/(T+a3_d)))\n",
    "\n",
    "x1 = [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0]\n",
    "p = [28.1,34.4,36.7,36.9,36.8,36.7,36.5,35.4,32.9,27.7,17.5]\n",
    "#d = lambda x1,A12,A21: x1*t.exp(A12*((A21*(1-x1))/(A12*x1 + A21*(1-x1)))**2)*p_satw + (1-x1)*t.exp(A21*((A12*x1)/(A12*x1+A21*(1-x1)))**2)*p_satd\n",
    "pd = lambda x1,A12,A21: x1*t.exp(A12*((A21*(1-x1))/(A12*x1 + A21*(1-x1)))**2)*p_satw + (1-x1)*t.exp(A21*((A12*x1)/(A12*x1+A21*(1-x1)))**2)*p_satd \n",
    "for i in range(len(x1)):    \n",
    "    loss = t.norm(p[i] - pd(x1[i],A[0],A[1]))**2\n",
    "    loss.backward()\n",
    "\n",
    "A.grad.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102dd828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement gradient descent to solve for A12 and A21\n",
    "# Wasn't able to solve because A.grad.numpy()[1] and [0] are too large so never meets error criteria, wasn't sure how to fix\n",
    "import torch as t\n",
    "from torch.autograd import Variable\n",
    "import math\n",
    "A = Variable(t.tensor([5.0,6.0]), requires_grad=True)\n",
    "T = 20\n",
    "a1_w = 8.07131\n",
    "a2_w = 1730.63\n",
    "a3_w = 233.426\n",
    "a1_d = 7.43155\n",
    "a2_d = 1554.679\n",
    "a3_d = 240.337\n",
    "p_satw = pow(10,(a1_w - (a2_w)/(T+a3_w)))\n",
    "\n",
    "p_satd = pow(10,(a1_d - (a2_d)/(T+a3_d)))\n",
    "\n",
    "x1 = [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0]\n",
    "p = [28.1,34.4,36.7,36.9,36.8,36.7,36.5,35.4,32.9,27.7,17.5]\n",
    "#d = lambda x1,A12,A21: x1*t.exp(A12*((A21*(1-x1))/(A12*x1 + A21*(1-x1)))**2)*p_satw + (1-x1)*t.exp(A21*((A12*x1)/(A12*x1+A21*(1-x1)))**2)*p_satd\n",
    "pd = lambda x1,A12,A21: x1*t.exp(A12*((A21*(1-x1))/(A12*x1 + A21*(1-x1)))**2)*p_satw + (1-x1)*t.exp(A21*((A12*x1)/(A12*x1+A21*(1-x1)))**2)*p_satd \n",
    "eps = 1e-7\n",
    "A12_0 = 5.0\n",
    "A21_0 = 6.0\n",
    "k1 = 0\n",
    "k2 = 0\n",
    "soln_1 = [A12_0]\n",
    "soln_2 = [A21_0]\n",
    "A12 = soln_1[k1]\n",
    "A21 = soln_2[k2]\n",
    "error = 1\n",
    "def line_search(A12,A21,x1):\n",
    "    a = 1\n",
    "    phi1 = lambda a, A12, A21, x1: pd(x1,A12,A21) - a*0.8*A.grad.numpy()[0]**2\n",
    "    phi2 = lambda a, A12, A21, x1: pd(x1,A12,A21) - a*0.8*A.grad.numpy()[1]**2\n",
    "    while phi1(a,A12,A21,x1)<pd(x1,A12-a*A.grad.numpy()[0],A21-a*A.grad.numpy()[1]) and phi2(a,A12,A21,x1)<pd(x1,A12-a*A.grad.numpy()[0],A21-a*A.grad.numpy()[1]):\n",
    "        a = 0.5*a\n",
    "    return a\n",
    "   \n",
    "    #loss = t.norm(p[i] - pd(x1[i],A[0],A[1]))**2\n",
    "    #loss.backward()\n",
    "    #A.grad.numpy()\n",
    "    #error = math.sqrt(A.grad.numpy()[0]**2 + A.grad.numpy()[1]**2)\n",
    "while error >= eps:\n",
    "    for i in range(len(x1)): \n",
    "        # calculate loss\n",
    "        loss = t.norm(p[i] - pd(x1[i],A[0],A[1]))**2\n",
    "        # calculate derivatives\n",
    "        loss.backward()\n",
    "        # check gradient\n",
    "        A.grad.numpy()\n",
    "        # create a with the gradient\n",
    "    a = line_search(A[0],A[1],x1[i])\n",
    "    # create new value for A12 with a and gradient\n",
    "    A12 = A12 - a*A.grad.numpy()[0]\n",
    "    # create new value for A21 with a and gradient\n",
    "    A21 = A21 - a*A.grad.numpy()[1]\n",
    "    # save solutions\n",
    "    soln_1.append(A12)\n",
    "    soln_2.append(A21)\n",
    "    # calculate error\n",
    "    error = math.sqrt(A.grad.numpy()[0]**2 + A.grad.numpy()[1]**2)\n",
    "    # Reset initial guesses based off line search values\n",
    "    A = Variable(t.tensor([A12,A21]), requires_grad=True)   \n",
    "soln_1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "id": "a05c27a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1093.2698 -1495.6486]\n",
      "0.00071549066\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5243201255884974,\n",
       " 1182.974207894897,\n",
       " 1346.8899999999996,\n",
       " 1361.61,\n",
       " 1354.2399999999998,\n",
       " 1346.89,\n",
       " 1332.25,\n",
       " 1253.1599999999999,\n",
       " 1082.4099999999999,\n",
       " 767.2899952834708,\n",
       " 0.0007154509784028096]"
      ]
     },
     "execution_count": 380,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# do gradient descent without line search\n",
    "import torch as t\n",
    "from torch.autograd import Variable\n",
    "import math\n",
    "A = Variable(t.tensor([5.0,6.0]), requires_grad=True)\n",
    "T = 20\n",
    "a1_w = 8.07131\n",
    "a2_w = 1730.63\n",
    "a3_w = 233.426\n",
    "a1_d = 7.43155\n",
    "a2_d = 1554.679\n",
    "a3_d = 240.337\n",
    "p_satw = pow(10,(a1_w - (a2_w)/(T+a3_w)))\n",
    "\n",
    "p_satd = pow(10,(a1_d - (a2_d)/(T+a3_d)))\n",
    "\n",
    "x1 = [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0]\n",
    "p = [28.1,34.4,36.7,36.9,36.8,36.7,36.5,35.4,32.9,27.7,17.5]\n",
    "#d = lambda x1,A12,A21: x1*t.exp(A12*((A21*(1-x1))/(A12*x1 + A21*(1-x1)))**2)*p_satw + (1-x1)*t.exp(A21*((A12*x1)/(A12*x1+A21*(1-x1)))**2)*p_satd\n",
    "pd = lambda x1,A12,A21: x1*t.exp(A12*((A21*(1-x1))/(A12*x1 + A21*(1-x1)))**2)*p_satw + (1-x1)*t.exp(A21*((A12*x1)/(A12*x1+A21*(1-x1)))**2)*p_satd \n",
    "for i in range(len(x1)):    \n",
    "    loss = t.norm(p[i] - pd(x1[i],A[0],A[1]))**2\n",
    "    loss.backward()\n",
    "\n",
    "A.grad.numpy()\n",
    "\n",
    "a = 0.01\n",
    "\n",
    "for j in range(11):\n",
    "    for i in range(len(x1)):\n",
    "        loss = t.norm(p[i] - pd(x1[i],A[0],A[1]))**2\n",
    "    loss.backward()\n",
    "    \n",
    "    with t.no_grad():\n",
    "        A -= a * A.grad\n",
    "        \n",
    "        A.grad.zero_()\n",
    "    \n",
    "print(A.data.numpy())\n",
    "print(loss.data.numpy())\n",
    "pdl = []\n",
    "error = []\n",
    "pd1 = lambda x1,A12,A21: x1*math.exp(A12*((A21*(1-x1))/(A12*x1 + A21*(1-x1)))**2)*p_satw + (1-x1)*math.exp(A21*((A12*x1)/(A12*x1+A21*(1-x1)))**2)*p_satd\n",
    "for i in range(len(x1)):\n",
    "    pdl.append(pd1(x1[i],A.data.numpy()[0],A.data.numpy()[1]))\n",
    "    error.append((p[i] - pd1(x1[i],A.data.numpy()[0],A.data.numpy()[1]))**2)\n",
    "pdl\n",
    "error\n",
    "\n",
    "# Here the error between the values calculated by using the formula and the values from the given points is very large, if the gradient descent method had worked, would have repeateed this same process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "id": "9495585a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.93813997, -1.05126187],\n",
       "        [-0.62037868,  1.2353595 ],\n",
       "        [ 2.47124112, -1.32560989],\n",
       "        [-1.04428165, -0.7245698 ],\n",
       "        [ 3.        , -2.        ],\n",
       "        [ 1.85248226,  0.84747227]]),\n",
       " array([  3.57209653,   3.69266144,  24.07589448,   2.05614108,\n",
       "        150.9       ,   3.22760183]))"
      ]
     },
     "execution_count": 372,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# utilized code written by Thomas Huijskens for Bayesian Optimization\n",
    "\n",
    "import numpy as np\n",
    "import sklearn.gaussian_process as gp\n",
    "\n",
    "from scipy.stats import norm\n",
    "from scipy.optimize import minimize\n",
    "#value = [np.linspace(-3,3, num=5), np.linspace(-2,2, num=5)]\n",
    "bounds = np.array([[-3,3], [-2,2]])\n",
    "n_iters = 2\n",
    "sample_loss = lambda value: (4 - 2.1*value[0]**2 + (value[0]**4)/3)*(value[0]**2) + value[0]*value[1] + (-4+4*(value[1]**2))*(value[1]**2)\n",
    "x0 = None\n",
    "n_pre_samples = 4\n",
    "gp_params = None\n",
    "random_search = False\n",
    "alpha = 1e-5\n",
    "epsilon = 1e-7\n",
    "#def bayesian_optimisation(n_iters, sample_loss, bounds, x0=None, n_pre_samples=5,\n",
    "                          #gp_params=None, random_search=False, alpha=1e-5, epsilon=1e-7):\n",
    "    \n",
    "x_list = []\n",
    "y_list = []\n",
    "\n",
    "n_params = bounds.shape[0]\n",
    "\n",
    "if x0 is None:\n",
    "    for params in np.random.uniform(bounds[:, 0], bounds[:, 1], (n_pre_samples, bounds.shape[0])):\n",
    "        x_list.append(params)\n",
    "        y_list.append(sample_loss(params))\n",
    "else:\n",
    "    for params in x0:\n",
    "        x_list.append(params)\n",
    "        y_list.append(sample_loss(params))\n",
    "\n",
    "xp = np.array(x_list)\n",
    "yp = np.array(y_list)\n",
    "\n",
    "    # Create the GP\n",
    "if gp_params is not None:\n",
    "    model = gp.GaussianProcessRegressor(**gp_params)\n",
    "else:\n",
    "    kernel = gp.kernels.Matern()\n",
    "    model = gp.GaussianProcessRegressor(kernel=kernel,\n",
    "                                            alpha=alpha,\n",
    "                                            n_restarts_optimizer=10,\n",
    "                                            normalize_y=True)\n",
    "\n",
    "for n in range(n_iters):\n",
    "\n",
    "    model.fit(xp, yp)\n",
    "\n",
    "        # Sample next hyperparameter\n",
    "    if random_search:\n",
    "        x_random = np.random.uniform(bounds[:, 0], bounds[:, 1], size=(random_search, n_params))\n",
    "        ei = -1 * expected_improvement(x_random, model, yp, greater_is_better=True, n_params=n_params)\n",
    "        next_sample = x_random[np.argmax(ei), :]\n",
    "    else:\n",
    "        next_sample = sample_next_hyperparameter(expected_improvement, model, yp, greater_is_better=True, bounds=bounds, n_restarts=100)\n",
    "\n",
    "        # Duplicates will break the GP. In case of a duplicate, we will randomly sample a next query point.\n",
    "    if np.any(np.abs(next_sample - xp) <= epsilon):\n",
    "        next_sample = np.random.uniform(bounds[:, 0], bounds[:, 1], bounds.shape[0])\n",
    "\n",
    "        # Sample loss for new set of parameters\n",
    "    cv_score = sample_loss(next_sample)\n",
    "\n",
    "        # Update lists\n",
    "    x_list.append(next_sample)\n",
    "    y_list.append(cv_score)\n",
    "\n",
    "        # Update xp and yp\n",
    "    xp = np.array(x_list)\n",
    "    yp = np.array(y_list)\n",
    "\n",
    "xp, yp\n",
    "# note xp = x1 and yp = x2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
